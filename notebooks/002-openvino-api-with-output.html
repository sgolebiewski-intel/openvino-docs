
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>OpenVINO™ Runtime API Tutorial &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    <script src="../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/tabs.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/custom.js"></script>
    <script src="../_static/js/graphs.js"></script>
    <script src="../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/notebooks/002-openvino-api-with-output.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hello Image Segmentation" href="003-hello-segmentation-with-output.html" />
    <link rel="prev" title="Hello Image Classification" href="001-hello-world-with-output.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../pages/resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notebooks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks-installation.html">
   Installation of OpenVINO™ Notebooks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="001-hello-world-with-output.html">
   Hello Image Classification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   OpenVINO™ Runtime API Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="003-hello-segmentation-with-output.html">
   Hello Image Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="004-hello-detection-with-output.html">
   Hello Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="101-tensorflow-to-openvino-with-output.html">
   Convert a TensorFlow Model to OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="102-pytorch-onnx-to-openvino-with-output.html">
   Convert a PyTorch Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="103-paddle-onnx-to-openvino-classification-with-output.html">
   Convert a PaddlePaddle Model to ONNX and OpenVINO™ IR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="104-model-tools-with-output.html">
   Working with Open Model Zoo Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="105-language-quantize-bert-with-output.html">
   Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="106-auto-device-with-output.html">
   Automatic Device Selection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="107-speech-recognition-quantization-with-output.html">
   Quantize Speech Recognition Models with OpenVINO™ Post-Training Optimization Tool ​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-nncf-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="110-ct-segmentation-quantize-with-output.html">
   Quantize a Segmentation Model and Show Live Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="111-detection-quantization-with-output.html">
   Object Detection Quantization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="112-pytorch-post-training-quantization-nncf-with-output.html">
   Post-Training Quantization of PyTorch models with NNCF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="113-image-classification-quantization-with-output.html">
   Quantization of Image Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="114-quantization-simplified-mode-with-output.html">
   INT8 Quantization with Post-training Optimization Tool (POT) in Simplified Mode tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="115-async-api-with-output.html">
   Asynchronous Inference with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="201-vision-monodepth-with-output.html">
   Monodepth Estimation with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-image-with-output.html">
   Single Image Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="202-vision-superresolution-video-with-output.html">
   Video Super Resolution with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="203-meter-reader-with-output.html">
   Industrial Meter Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="204-named-entity-recognition-with-output.html">
   Document Entity Extraction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="205-vision-background-removal-with-output.html">
   Image Background Removal with U^2-Net and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="206-vision-paddlegan-anime-with-output.html">
   Photos to Anime with PaddleGAN and OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="207-vision-paddlegan-superresolution-with-output.html">
   Super Resolution with PaddleGAN and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="208-optical-character-recognition-with-output.html">
   Optical Character Recognition (OCR) with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="209-handwritten-ocr-with-output.html">
   Handwritten Chinese and Japanese OCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="210-ct-scan-live-inference-with-output.html">
   Live Inference and Benchmark CT-scan Data with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="211-speech-to-text-with-output.html">
   Speech to Text with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="212-onnx-style-transfer-with-output.html">
   Style Transfer on ONNX Models with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="214-vision-paddle-classification-with-output.html">
   PaddlePaddle Image Classification with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="215-image-inpainting-with-output.html">
   Image In-painting with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="216-license-plate-recognition-with-output.html">
   License Plate Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="217-vision-deblur-with-output.html">
   Deblur Photos with DeblurGAN-v2 and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="218-vehicle-detection-and-recognition-with-output.html">
   Vehicle Detection And Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="219-knowledge-graphs-conve-with-output.html">
   OpenVINO optimizations for Knowledge graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="220-yolov5-accuracy-check-and-quantization-with-output.html">
   Quantize the Ultralytics YOLOv5 model and check accuracy using the OpenVINO POT API
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="221-machine-translation-with-output.html">
   Machine translation demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="222-vision-image-colorization-with-output.html">
   Image Colorization with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="223-gpt2-text-prediction-with-output.html">
   GPT-2 Text Prediction with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-pot-with-output.html">
   Post-Training Quantization with TensorFlow Classification Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="301-tensorflow-training-openvino-with-output.html">
   From Training to Deployment with TensorFlow and OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="302-pytorch-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using PyTorch framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="305-tensorflow-quantization-aware-training-with-output.html">
   Quantization Aware Training with NNCF, using TensorFlow Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="401-object-detection-with-output.html">
   Live Object Detection with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="402-pose-estimation-with-output.html">
   Live Human Pose Estimation with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="403-action-recognition-webcam-with-output.html">
   Human Action Recognition with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="405-paddle-ocr-webcam-with-output.html">
   PaddleOCR with OpenVINO™
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebook_utils-with-output.html">
   Notebook Utils
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-openvino-runtime-and-showing-info">
   Loading OpenVINO Runtime and Showing Info
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-a-model">
   Loading a Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#openvino-ir-model">
     OpenVINO IR Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-model">
     ONNX Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-information-about-a-model">
   Getting Information about a Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-inputs">
     Model Inputs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-outputs">
     Model Outputs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#doing-inference-on-a-model">
   Doing Inference on a Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reshaping-and-resizing">
   Reshaping and Resizing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#change-image-size">
     Change Image Size
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#change-batch-size">
     Change Batch Size
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#caching-a-model">
   Caching a Model
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <section id="openvino-runtime-api-tutorial">
<h1>OpenVINO™ Runtime API Tutorial<a class="headerlink" href="#openvino-runtime-api-tutorial" title="Permalink to this headline">¶</a></h1>
<p>This notebook explains the basics of the OpenVINO Runtime API. It
covers:</p>
<ul class="simple">
<li><p><a class="reference external" href="#Loading-OpenVINO-Runtime-and-Showing-Info">Loading OpenVINO Runtime and Showing
Info</a></p></li>
<li><p><a class="reference external" href="#Loading-a-Model">Loading a Model</a></p>
<ul>
<li><p><a class="reference external" href="#OpenVINO-IR-Model">OpenVINO IR Model</a></p></li>
<li><p><a class="reference external" href="#ONNX-Model">ONNX Model</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#Getting-Information-about-a-Model">Getting Information about a
Model</a></p>
<ul>
<li><p><a class="reference external" href="#Model-Inputs">Model Inputs</a></p></li>
<li><p><a class="reference external" href="#Model-Outputs">Model Outputs</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#Doing-Inference-on-a-Model">Doing Inference on a Model</a></p></li>
<li><p><a class="reference external" href="#Reshaping-and-Resizing">Reshaping and Resizing</a></p>
<ul>
<li><p><a class="reference external" href="#Change-Image-Size">Change Image Size</a></p></li>
<li><p><a class="reference external" href="#Change-Batch-Size">Change Batch Size</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#Caching-a-Model">Caching a Model</a></p></li>
</ul>
<p>The notebook is divided into sections with headers. Each section is
standalone and does not depend on previous sections. A segmentation and
classification OpenVINO IR model and a segmentation ONNX model are
provided as examples. These model files can be replaced with your own
models. The exact outputs will be different, but the process is the
same.</p>
<section id="loading-openvino-runtime-and-showing-info">
<h2>Loading OpenVINO Runtime and Showing Info<a class="headerlink" href="#loading-openvino-runtime-and-showing-info" title="Permalink to this headline">¶</a></h2>
<p>Initialize OpenVINO Runtime with Core()</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
</pre></div>
</div>
<p>OpenVINO Runtime can load a network on a device. A device in this
context means a CPU, an Intel GPU, a Neural Compute Stick 2, etc. The
<code class="docutils literal notranslate"><span class="pre">available_devices</span></code> property shows the available devices in your
system. The “FULL_DEVICE_NAME” option to <code class="docutils literal notranslate"><span class="pre">ie.get_property()</span></code> shows the
name of the device.</p>
<p>In this notebook, the CPU device is used. To use an integrated GPU, use
<code class="docutils literal notranslate"><span class="pre">device_name=&quot;GPU&quot;</span></code> instead. Be aware that loading a network on GPU
will be slower than loading a network on CPU, but inference will likely
be faster.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">devices</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">available_devices</span>

<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
    <span class="n">device_name</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">get_property</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="s2">&quot;FULL_DEVICE_NAME&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span> <span class="n">i9</span><span class="o">-</span><span class="mi">10920</span><span class="n">X</span> <span class="n">CPU</span> <span class="o">@</span> <span class="mf">3.50</span><span class="n">GHz</span>
</pre></div>
</div>
</section>
<section id="loading-a-model">
<h2>Loading a Model<a class="headerlink" href="#loading-a-model" title="Permalink to this headline">¶</a></h2>
<p>After initializing OpenVINO Runtime, first read the model file with
<code class="docutils literal notranslate"><span class="pre">read_model()</span></code>, then compile it to the specified device with the
<code class="docutils literal notranslate"><span class="pre">compile_model()</span></code> method.</p>
<section id="openvino-ir-model">
<h3>OpenVINO IR Model<a class="headerlink" href="#openvino-ir-model" title="Permalink to this headline">¶</a></h3>
<p>An OpenVINO IR (Intermediate Representation) model consists of an
<code class="docutils literal notranslate"><span class="pre">.xml</span></code> file, containing information about network topology, and a
<code class="docutils literal notranslate"><span class="pre">.bin</span></code> file, containing the weights and biases binary data. The
<code class="docutils literal notranslate"><span class="pre">read_model()</span></code> function expects the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> weights file to have the
same filename and be located in the same directory as the <code class="docutils literal notranslate"><span class="pre">.xml</span></code> file:
<code class="docutils literal notranslate"><span class="pre">model_weights_file</span> <span class="pre">==</span> <span class="pre">Path(model_xml).with_suffix(&quot;.bin&quot;)</span></code>. If this
is the case, specifying the weights file is optional. If the weights
file has a different filename, it can be specified with the <code class="docutils literal notranslate"><span class="pre">weights</span></code>
parameter to <code class="docutils literal notranslate"><span class="pre">read_model()</span></code>.</p>
<p>For information on how to convert your existing TensorFlow, PyTorch or
ONNX model to OpenVINO IR format with Model Optimizer, refer to the
<a class="reference external" href="101-tensorflow-to-openvino-with-output.html">tensorflow-to-openvino</a>
and
<a class="reference external" href="102-pytorch-onnx-to-openvino-with-output.html">pytorch-onnx-to-openvino</a>
notebooks. For exporting ONNX models to OpenVINO IR with default
settings, the <code class="docutils literal notranslate"><span class="pre">.serialize()</span></code> method can also be used.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="onnx-model">
<h3>ONNX Model<a class="headerlink" href="#onnx-model" title="Permalink to this headline">¶</a></h3>
<p>Reading and loading an ONNX model, which is a single file, works the
same way as with an OpenVINO IR model. The <code class="docutils literal notranslate"><span class="pre">model</span></code> argument points to
the filename of an ONNX model.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">onnx_model_path</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.onnx&quot;</span>
<span class="n">model_onnx</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">)</span>
<span class="n">compiled_model_onnx</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_onnx</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The ONNX model can be exported to OpenVINO IR with <code class="docutils literal notranslate"><span class="pre">.serialize()</span></code>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.offline_transformations</span> <span class="kn">import</span> <span class="n">serialize</span>

<span class="n">serialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_onnx</span><span class="p">,</span> <span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;model/exported_onnx_model.xml&quot;</span><span class="p">,</span> <span class="n">weights_path</span><span class="o">=</span><span class="s2">&quot;model/exported_onnx_model.bin&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="getting-information-about-a-model">
<h2>Getting Information about a Model<a class="headerlink" href="#getting-information-about-a-model" title="Permalink to this headline">¶</a></h2>
<p>The OpenVINO IENetwork instance stores information about the model.
Information about the inputs and outputs of the model are in
<code class="docutils literal notranslate"><span class="pre">model.inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">model.outputs</span></code>. These are also properties of the
ExecutableNetwork instance. While using <code class="docutils literal notranslate"><span class="pre">model.inputs</span></code> and
<code class="docutils literal notranslate"><span class="pre">model.outputs</span></code> in the cells below, you can also use
<code class="docutils literal notranslate"><span class="pre">compiled_model.inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">compiled_model.outputs</span></code>.</p>
<section id="model-inputs">
<h3>Model Inputs<a class="headerlink" href="#model-inputs" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any_name</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;input&#39;</span>
</pre></div>
</div>
<p>The cell above shows that the model loaded expects one input, with the
name <em>input</em>. If you loaded a different model, you may see a different
input layer name, and you may see more inputs.</p>
<p>It is often useful to have a reference to the name of the first input
layer. For a model with one input, <code class="docutils literal notranslate"><span class="pre">model.input(0)</span></code> gets this name.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Information for this input layer is stored in <code class="docutils literal notranslate"><span class="pre">inputs</span></code>. The next cell
prints the input layout, precision and a shape.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input precision: </span><span class="si">{</span><span class="n">input_layer</span><span class="o">.</span><span class="n">element_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input shape: </span><span class="si">{</span><span class="n">input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">precision</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">Type</span><span class="p">:</span> <span class="s1">&#39;float32&#39;</span><span class="o">&gt;</span>
<span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">}</span>
</pre></div>
</div>
<p>This cell shows that the model expects inputs with a shape of
[1,3,224,224], and that this is in the <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> layout. This means that
the model expects input data with the batch size of 1 (<code class="docutils literal notranslate"><span class="pre">N</span></code>), 3
channels (<code class="docutils literal notranslate"><span class="pre">C</span></code>) , and images with a height (<code class="docutils literal notranslate"><span class="pre">H</span></code>) and width (<code class="docutils literal notranslate"><span class="pre">W</span></code>)
equal to 224. The input data is expected to be of <code class="docutils literal notranslate"><span class="pre">FP32</span></code> (floating
point) precision.</p>
</section>
<section id="model-outputs">
<h3>Model Outputs<a class="headerlink" href="#model-outputs" title="Permalink to this headline">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any_name</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;MobilenetV3/Predictions/Softmax&#39;</span>
</pre></div>
</div>
<p>Model output info is stored in <code class="docutils literal notranslate"><span class="pre">model.outputs</span></code>. The cell above shows
that the model returns one output, with the
<code class="docutils literal notranslate"><span class="pre">MobilenetV3/Predictions/Softmax</span></code> name. Loading a different model will
result in different output layer name, and more outputs might be
returned.</p>
<p>Since this model has one output, follow the same method as for the input
layer to get its name.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output_layer</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">Output</span><span class="p">:</span> <span class="n">names</span><span class="p">[</span><span class="n">MobilenetV3</span><span class="o">/</span><span class="n">Predictions</span><span class="o">/</span><span class="n">Softmax</span><span class="p">]</span> <span class="n">shape</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">1001</span><span class="p">}</span> <span class="nb">type</span><span class="p">:</span> <span class="n">f32</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Getting the output precision and shape is similar to getting the input
precision and shape.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output precision: </span><span class="si">{</span><span class="n">output_layer</span><span class="o">.</span><span class="n">element_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output shape: </span><span class="si">{</span><span class="n">output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="n">precision</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">Type</span><span class="p">:</span> <span class="s1">&#39;float32&#39;</span><span class="o">&gt;</span>
<span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1001</span><span class="p">}</span>
</pre></div>
</div>
<p>This cell shows that the model returns outputs with a shape of [1,
1001], where 1 is the batch size (<code class="docutils literal notranslate"><span class="pre">N</span></code>) and 1001 is the number of
classes (<code class="docutils literal notranslate"><span class="pre">C</span></code>). The output is returned as 32-bit floating point.</p>
</section>
</section>
<section id="doing-inference-on-a-model">
<h2>Doing Inference on a Model<a class="headerlink" href="#doing-inference-on-a-model" title="Permalink to this headline">¶</a></h2>
<p>To do inference on a model, first create an inference request by calling
the <code class="docutils literal notranslate"><span class="pre">create_infer_request()</span></code> method of <code class="docutils literal notranslate"><span class="pre">ExecutableNetwork</span></code>,
<code class="docutils literal notranslate"><span class="pre">exec_net</span></code> that was loaded with <code class="docutils literal notranslate"><span class="pre">compile_model()</span></code>. Then, call the
<code class="docutils literal notranslate"><span class="pre">infer()</span></code> method of <code class="docutils literal notranslate"><span class="pre">InferRequest</span></code>. It expects one argument:
<code class="docutils literal notranslate"><span class="pre">inputs</span></code>. This is a dictionary that maps input layer names to input
data.</p>
<p><strong>Load the network</strong></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load an image and convert to the input shape</strong></p>
<p>To propagate an image through the network, it needs to be loaded into an
array, resized to the shape that the network expects, and converted to
the input layout of the network.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">image_filename</span> <span class="o">=</span> <span class="s2">&quot;data/coco_hollywood.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_filename</span><span class="p">)</span>
<span class="n">image</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">663</span><span class="p">,</span> <span class="mi">994</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>The image has a shape of (663,994,3). It is 663 pixels in height, 994
pixels in width, and has 3 color channels. A reference to the height and
width expected by the network is obtained and the image is resized to
these dimensions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># N,C,H,W = batch size, number of channels, height, width.</span>
<span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># OpenCV resize expects the destination size as (width, height).</span>
<span class="n">resized_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">dsize</span><span class="o">=</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
<span class="n">resized_image</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, the image has the width and height that the network expects. This
is still in <code class="docutils literal notranslate"><span class="pre">HWC</span></code> format and must be changed to <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> format.
First, call the <code class="docutils literal notranslate"><span class="pre">np.transpose()</span></code> method to change to <code class="docutils literal notranslate"><span class="pre">CHW</span></code> and then
add the <code class="docutils literal notranslate"><span class="pre">N</span></code> dimension (where <code class="docutils literal notranslate"><span class="pre">N</span></code>= 1) by calling the
<code class="docutils literal notranslate"><span class="pre">np.expand_dims()</span></code> method. Next, convert the data to <code class="docutils literal notranslate"><span class="pre">FP32</span></code> with
<code class="docutils literal notranslate"><span class="pre">np.astype()</span></code> method.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">resized_image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">input_data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Do inference</strong></p>
<p>Now that the input data is in the right shape, do the inference.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="p">([</span><span class="n">input_data</span><span class="p">])[</span><span class="n">output_layer</span><span class="p">]</span>
</pre></div>
</div>
<p>You can also create <code class="docutils literal notranslate"><span class="pre">InferRequest</span></code> and run <code class="docutils literal notranslate"><span class="pre">infer</span></code> method on
request.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">request</span> <span class="o">=</span> <span class="n">compiled_model</span><span class="o">.</span><span class="n">create_infer_request</span><span class="p">()</span>
<span class="n">request</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="n">input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">get_output_tensor</span><span class="p">(</span><span class="n">output_layer</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.infer()</span></code> function sets output tensor, that can be reached, using
<code class="docutils literal notranslate"><span class="pre">get_output_tensor()</span></code>. Since this network returns one output, and the
reference to the output layer is in the <code class="docutils literal notranslate"><span class="pre">output_layer.index</span></code>
parameter, you can get the data with
<code class="docutils literal notranslate"><span class="pre">request.get_output_tensor(output_layer.index)</span></code>. To get a numpy array
from the output, use the <code class="docutils literal notranslate"><span class="pre">.data</span></code> parameter.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
</pre></div>
</div>
<p>The output shape is (1,1001), which is the expected output shape. This
shape indicates that the network returns probabilities for 1001 classes.
To learn more about this notion, refer to the <a class="reference external" href="001-hello-world-with-output.html">hello world
notebook</a>.</p>
</section>
<section id="reshaping-and-resizing">
<h2>Reshaping and Resizing<a class="headerlink" href="#reshaping-and-resizing" title="Permalink to this headline">¶</a></h2>
<section id="change-image-size">
<h3>Change Image Size<a class="headerlink" href="#change-image-size" title="Permalink to this headline">¶</a></h3>
<p>Instead of reshaping the image to fit the model, it is also possible to
reshape the model to fit the image. Be aware that not all models support
reshaping, and models that do, may not support all input shapes. The
model accuracy may also suffer if you reshape the model input shape.</p>
<p>First check the input shape of the model, then reshape it to the new
input shape.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">segmentation_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.xml&quot;</span>
<span class="n">segmentation_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model_xml</span><span class="p">)</span>
<span class="n">segmentation_input_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">segmentation_output_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~~~~ ORIGINAL MODEL ~~~~&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input shape: </span><span class="si">{</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">new_shape</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">])</span>
<span class="n">segmentation_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">new_shape</span><span class="p">})</span>
<span class="n">segmentation_compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="c1"># help(segmentation_compiled_model)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~~~~ RESHAPED MODEL ~~~~&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model input shape: </span><span class="si">{</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;compiled_model input shape: &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">segmentation_compiled_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;compiled_model output shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">~~~~</span> <span class="n">ORIGINAL</span> <span class="n">MODEL</span> <span class="o">~~~~</span>
<span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">}</span>
<span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">}</span>
<span class="o">~~~~</span> <span class="n">RESHAPED</span> <span class="n">MODEL</span> <span class="o">~~~~</span>
<span class="n">model</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
<span class="n">compiled_model</span> <span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
<span class="n">compiled_model</span> <span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
</pre></div>
</div>
<p>The input shape for the segmentation network is [1,3,512,512], with the
<code class="docutils literal notranslate"><span class="pre">NCHW</span></code> layout: the network expects 3-channel images with a width and
height of 512 and a batch size of 1. Reshape the network with the
<code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> method of <code class="docutils literal notranslate"><span class="pre">IENetwork</span></code> to make it accept input images
with a width and height of 544. This segmentation network always returns
arrays with the input width and height of equal value. Therefore,
setting the input dimensions to 544x544 also modifies the output
dimensions. After reshaping, compile the network once again.</p>
</section>
<section id="change-batch-size">
<h3>Change Batch Size<a class="headerlink" href="#change-batch-size" title="Permalink to this headline">¶</a></h3>
<p>Use the <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> method to set the batch size, by increasing the
first element of <code class="docutils literal notranslate"><span class="pre">new_shape</span></code>. For example, to set a batch size of two,
set <code class="docutils literal notranslate"><span class="pre">new_shape</span> <span class="pre">=</span> <span class="pre">(2,3,544,544)</span></code> in the cell above.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">segmentation_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.xml&quot;</span>
<span class="n">segmentation_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model_xml</span><span class="p">)</span>
<span class="n">segmentation_input_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">segmentation_output_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">new_shape</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">])</span>
<span class="n">segmentation_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">new_shape</span><span class="p">})</span>
<span class="n">segmentation_compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input shape: </span><span class="si">{</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
<span class="n">output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
</pre></div>
</div>
<p>The output shows that by setting the batch size to 2, the first element
(<code class="docutils literal notranslate"><span class="pre">N</span></code>) of the input and output shape has a value of 2. Propagate the
input image through the network to see the result:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
<span class="n">segmentation_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/segmentation.xml&quot;</span>
<span class="n">segmentation_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model_xml</span><span class="p">)</span>
<span class="n">segmentation_input_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">segmentation_output_layer</span> <span class="o">=</span> <span class="n">segmentation_model</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">new_shape</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">])</span>
<span class="n">segmentation_model</span><span class="o">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">segmentation_input_layer</span><span class="o">.</span><span class="n">any_name</span><span class="p">:</span> <span class="n">new_shape</span><span class="p">})</span>
<span class="n">segmentation_compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">segmentation_model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">segmentation_compiled_model</span><span class="p">([</span><span class="n">input_data</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input data shape: </span><span class="si">{</span><span class="n">input_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;result data data shape: </span><span class="si">{</span><span class="n">segmentation_output_layer</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">data</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">)</span>
<span class="n">result</span> <span class="n">data</span> <span class="n">data</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">544</span><span class="p">,</span> <span class="mi">544</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="caching-a-model">
<h2>Caching a Model<a class="headerlink" href="#caching-a-model" title="Permalink to this headline">¶</a></h2>
<p>For some devices, like GPU, loading a model can take some time. Model
Caching solves this issue by caching the model in a cache directory. If
<code class="docutils literal notranslate"><span class="pre">ie.compile_model(model=net,</span> <span class="pre">device_name=device_name,</span> <span class="pre">config=config_dict)</span></code>
is set, caching will be used. This option checks if a model exists in
the cache. If so, it loads it from the cache. If not, it loads the model
regularly, and stores it in the cache, so that the next time the model
is loaded when this option is set, the model will be loaded from the
cache.</p>
<p>In the cell below, we create a <em>model_cache</em> directory as a subdirectory
of <em>model</em>, where the model will be cached for the specified device. The
model will be loaded to the GPU. After running this cell once, the model
will be cached, so subsequent runs of this cell will load the model from
the cache.</p>
<p><em>Note: Model Caching is not available on CPU devices</em></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">openvino.runtime</span> <span class="kn">import</span> <span class="n">Core</span><span class="p">,</span> <span class="n">PartialShape</span>

<span class="n">ie</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>

<span class="n">device_name</span> <span class="o">=</span> <span class="s2">&quot;GPU&quot;</span>  <span class="c1"># Model Caching is not available for CPU</span>

<span class="k">if</span> <span class="n">device_name</span> <span class="ow">in</span> <span class="n">ie</span><span class="o">.</span><span class="n">available_devices</span> <span class="ow">and</span> <span class="n">device_name</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
    <span class="n">cache_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;model/model_cache&quot;</span><span class="p">)</span>
    <span class="n">cache_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Enable caching for OpenVINO Runtime. To disable caching set enable_caching = False</span>
    <span class="n">enable_caching</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;CACHE_DIR&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">cache_path</span><span class="p">)}</span> <span class="k">if</span> <span class="n">enable_caching</span> <span class="k">else</span> <span class="p">{}</span>

    <span class="n">classification_model_xml</span> <span class="o">=</span> <span class="s2">&quot;model/classification.xml&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">classification_model_xml</span><span class="p">)</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="n">device_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_dict</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading the network to the </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2"> device took </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model caching is not available on CPU devices.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span> <span class="n">caching</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span> <span class="n">on</span> <span class="n">CPU</span> <span class="n">devices</span><span class="o">.</span>
</pre></div>
</div>
<p>After running the previous cell, we know the model exists in the cache
directory. We delete the compiled model and load it again. We measure
the time it takes now.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">device_name</span> <span class="ow">in</span> <span class="n">ie</span><span class="o">.</span><span class="n">available_devices</span> <span class="ow">and</span> <span class="n">device_name</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
    <span class="k">del</span> <span class="n">compiled_model</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">ie</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="n">device_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_dict</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading the network to the </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2"> device took </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="001-hello-world-with-output.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="003-hello-segmentation-with-output.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>