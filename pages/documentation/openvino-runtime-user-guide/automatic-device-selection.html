
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="The Automatic Device Selection mode in OpenVINO™ Runtime detects available devices and selects the optimal processing unit for inference automatically." name="description" />
<meta content="OpenVINO™ Runtime, Automatic Device Selection, Automatic Device Selection mode, AUTO device, AUTO plugin, inference, model inference, processing unit, input model, precision, model precision, FP32, FP16, FP16 precision, INT8, INT8 precision, BIN, dGPU, iGPU, Intel® Movidius™ Myriad™ X VPU, Intel® CPU, device candidate list, performance hints, throughput, latency, cumulative_throughput, model priority, disable auto-batching, benchmark_app" name="keywords" />

    <title>Automatic Device Selection &#8212; OpenVINO™  documentation</title>
    
    
  <link href="../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="../../../_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="../../../_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/tabs.css" type="text/css" />
    <script src="../../../_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="../../../_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="../../../_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/tabs.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/custom.js"></script>
    <script src="../../../_static/js/graphs.js"></script>
    <script src="../../../_static/js/graphs_ov_tf.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/target-highlight.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="canonical" href="https://docs.openvino.ai/latest/pages/documentation/openvino-runtime-user-guide/automatic-device-selection.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Debugging Auto-Device Plugin" href="automatic-device-selection/debugging-auto-device.html" />
    <link rel="prev" title="When Dynamic Shapes API is Not Applicable" href="dynamic-shapes/dynamic-shapes-not-applicable.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/logo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../get-started-guide.html">
  Get Started
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../../documentation.html">
  Documentation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../api/api_reference.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../model_zoo.html">
  Model Zoo
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../resources.html">
  Resources
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/openvinotoolkit/openvino" rel="noopener" target="_blank" title="GitHub">
            <span><i class="sst-github"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="language-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">English</button>
  <div class="dropdown-menu" aria-labelledby="language-selector">
    
      
        <a class="dropdown-item font-weight-bold" href="/openvino-docs/index.html">English</a>
      
    
      
        <a  class="dropdown-item" href="/cn/openvino-docs/index.html">Chinese</a>
      
    
  </div>
</div>

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="../../../search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API 2.0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../api-2.0-transition.html">
   OpenVINO™ API 2.0 Transition Guide
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../api-2.0-transition/api-2.0-deployment.html">
     Installation &amp; Deployment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api-2.0-transition/api-2.0-inference-pipeline.html">
     Inference Pipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api-2.0-transition/api-2.0-configure-devices.html">
     Configuring Devices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api-2.0-transition/api-2.0-preprocessing.html">
     Preprocessing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api-2.0-transition/api-2.0-model-creation.html">
     Model Creation in OpenVINO™ Runtime
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Converting and Preparing Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../model-processing.html">
   Introduction to Model Processing
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../converting-models-with-model-optimizer.html">
   Converting Models with Model Optimizer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/input-shapes.html">
     Setting Input Shapes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/cutting-model-with-model-optimizer.html">
     Cutting Off Parts of a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/additional-optimization-use-cases.html">
     Embedding Preprocessing Computation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/fp16-compression-with-model-optimizer.html">
     Compressing a Model to FP16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-tensorflow-model-with-model-optimizer.html">
     Converting a TensorFlow Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-onnx-model-with-model-optimizer.html">
     Converting an ONNX Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-pytorch-model.html">
     Converting a PyTorch Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-paddlepaddle-model-with-model-optimizer.html">
     Converting a PaddlePaddle Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-mxnet-model-with-model-optimizer.html">
     Converting an MXNet Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-caffe-model-with-model-optimizer.html">
     Converting a Caffe Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/converting-kaldi-model-with-model-optimizer.html">
     Converting a Kaldi Model
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials.html">
     Model Conversion Tutorials
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-attention-ocr-model.html">
       Convert a TensorFlow Attention OCR Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-bert-model.html">
       Convert a TensorFlow BERT Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-crnn-model.html">
       Convert a TensorFlow CRNN Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-deepspeech-model.html">
       Convert a TensorFlow DeepSpeech Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-efficientdet-model.html">
       Convert TensorFlow EfficientDet Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-facenet-model.html">
       Convert TensorFlow FaceNet Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-gnmt-model.html">
       Convert a TensorFlow GNMT Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-language-model-on-billion-word-benchmark.html">
       Convert a TensorFlow Language Model on One Billion Word Benchmark
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-ncf-model.html">
       Convert a TensorFlow Neural Collaborative Filtering Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-object-detection-api-model.html">
       Convert TensorFlow Object Detection API Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-retinanet-model.html">
       Convert a TensorFlow RetinaNet Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-slim-library-models.html">
       Convert TensorFlow Slim Image Classification Model Library Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-wide-and-deep-family-models.html">
       Convert TensorFlow Wide and Deep Family Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-xlnet-model.html">
       Convert a TensorFlow XLNet Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-tensorflow-yolo-models.html">
       Convert TensorFlow YOLO Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-onnx-faster-r-cnn-model.html">
       Convert an ONNX Faster R-CNN Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-onnx-gpt-2-model.html">
       Convert an ONNX GPT-2 Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-onnx-mask-r-cnn-model.html">
       Convert an ONNX Mask R-CNN Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-bert-ner-model.html">
       Convert a PyTorch BERT-NER Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-cascade-rcn-r-101-model.html">
       Convert a PyTorch Cascade RCNN R-101 Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-f3net-model.html">
       Convert a PyTorch F3Net Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-quartznet-model.html">
       Convert a PyTorch QuartzNet Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-rcan-model.html">
       Convert a PyTorch RCAN Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-rnn-t-model.html">
       Convert a PyTorch RNN-T Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-pytorch-yolact-model.html">
       Convert a PyTorch YOLACT Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-mxnet-gluoncv-models.html">
       Convert MXNet GluonCV Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-mxnet-style-transfer-model.html">
       Convert an MXNet Style Transfer Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../converting-models-with-model-optimizer/convert-model-tutorials/converting-kaldi-aspire-chain-tdnn-model.html">
       Convert a Kaldi ASpIRE Chain Time Delay Neural Network (TDNN) Model
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../converting-models-with-model-optimizer/faq-model-optimizer.html">
     Model Optimizer Frequently Asked Questions
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Optimization and Performance
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../performance-optimization.html">
   Introduction to Performance Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting-performance-numbers.html">
   Getting Performance Numbers
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../model-optimization-guide.html">
   Model Optimization Guide
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization.html">
     Optimizing models post-training
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/quantizing-model.html">
       Quantizing Model
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
      <label for="toctree-checkbox-6">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/quantizing-model/default-quantization-algorithm.html">
         DefaultQuantization Algorithm
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/accuracy-aware-quantization.html">
       Quantizing Model with Accuracy Control
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
      <label for="toctree-checkbox-7">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/accuracy-aware-quantization/accuracy-aware-quantization-algorithm.html">
         AccuracyAwareQuantization Algorithm
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/best-practices.html">
       Post-Training Quantization Best Practices
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
      <label for="toctree-checkbox-8">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/best-practices/saturation-issue.html">
         Saturation Issue
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/pot-api.html">
       Post-training Optimization Tool API
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/command-line-interface.html">
       POT Command-line Interface
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
      <label for="toctree-checkbox-9">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/command-line-interface/simplified-mode.html">
         Optimization in Simplified Mode
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/command-line-interface/configuration-file.html">
         Configuration File Description
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples.html">
       Examples
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
      <label for="toctree-checkbox-10">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4 has-children">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples.html">
         POT API Examples
        </a>
        <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
        <label for="toctree-checkbox-11">
         <i class="fas fa-chevron-down">
         </i>
        </label>
        <ul>
         <li class="toctree-l5">
          <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples/quantizing-image-classification-model.html">
           Quantizing Image Classification Model
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples/quantizing-with-accuracy-control.html">
           Quantizing with Accuracy Control
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples/quantizing-face-detection-model.html">
           Quantizing Face Detection Model
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples/quantizing-segmentation-model.html">
           Quantizing Semantic Segmentation Model
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples/quantizing-3d-segmentation-model.html">
           Quantizing 3D Segmentation Model
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/api-examples/quantizing-for-gna-device.html">
           Quantizing for GNA Device
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/examples/command-line-example.html">
         Command-line Interface Example
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../model-optimization-guide/post-training-model-optimization/post-training-optimization-tool-faq.html">
       Post-training Optimization Tool FAQ
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../model-optimization-guide/neural-network-compression-framework.html">
     Neural Network Compression Framework
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../model-optimization-guide/experimental-protecting-model.html">
     Experimental: Protecting Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../runtime-inference-optimizations.html">
   Runtime Inference Optimizations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../runtime-inference-optimizations/general-optimizations.html">
     General Optimizations
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../runtime-inference-optimizations/optimizing-for-latency.html">
     Optimizing for the Latency
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../runtime-inference-optimizations/optimizing-for-latency/model-caching-overview.html">
       Model Caching Overview
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../runtime-inference-optimizations/optimizing-for-throughput.html">
     Optimizing for Throughput
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../runtime-inference-optimizations/advanced-throughput-options.html">
     Advanced Throughput Options
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../runtime-inference-optimizations/further-low-level-implementation.html">
     Further Low-Level Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../tuning-utilities.html">
   Tuning Utilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../tuning-utilities/cross-check-tool.html">
     Cross Check Tool
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../performance-benchmarks.html">
   Performance Benchmarks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../performance-benchmarks/openvino-performance-benchmarks.html">
     Intel® Distribution of OpenVINO™ toolkit Benchmark Results
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
    <label for="toctree-checkbox-16">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../performance-benchmarks/openvino-performance-benchmarks/performance-benchmarks-faq.html">
       Performance Information Frequently Asked Questions
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../performance-benchmarks/openvino-performance-benchmarks/model-accuracy-for-int8-fp32.html">
       Model Accuracy for INT8 and FP32 Precision
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../performance-benchmarks/openvino-model-server-performance-benchmarks.html">
     OpenVINO™ Model Server Benchmark Results
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deploying Inference
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../openvino-deployment-introduction.html">
   Introduction to OpenVINO™ Deployment
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../openvino-runtime-user-guide.html">
   Performing Inference with OpenVINO Runtime
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="openvino-runtime-integrate-application.html">
     Integrate OpenVINO™ with Your Application
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
    <label for="toctree-checkbox-18">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="openvino-runtime-integrate-application/model-representation.html">
       Model Representation in OpenVINO™ Runtime
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="openvino-runtime-integrate-application/inference-request.html">
       OpenVINO™ Inference Request
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="openvino-runtime-integrate-application/python-api-exclusives.html">
       OpenVINO™ Python API Exclusives
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="changing-input-shapes.html">
     Changing Input Shapes
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="working-with-devices.html">
     Working with devices
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
    <label for="toctree-checkbox-19">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="working-with-devices/query-device-properties.html">
       Query Device Properties
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="working-with-devices/inference-device-cpu.html">
       CPU Device
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="working-with-devices/inference-device-gpu.html">
       GPU Device
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
      <label for="toctree-checkbox-20">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="working-with-devices/inference-device-gpu/remote-tensor-gpu.html">
         Remote Tensor API of GPU Plugin
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="working-with-devices/inference-device-vpu.html">
       VPU Devices
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
      <label for="toctree-checkbox-21">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="working-with-devices/inference-device-vpu/myriad-device.html">
         MYRIAD Device
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="working-with-devices/inference-device-vpu/hddl-device.html">
         HDDL Device
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="working-with-devices/inference-device-gna.html">
       GNA Device
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="working-with-devices/inference-device-arm-cpu.html">
       Arm® CPU Device
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="preprocessing.html">
     Preprocessing
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
    <label for="toctree-checkbox-22">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="preprocessing/preprocessing-api-details.html">
       Preprocessing API
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="preprocessing/layout-api-overview.html">
       Layout API Overview
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="preprocessing/preprocessing-use-case.html">
       Preprocessing - Use Case
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="dynamic-shapes.html">
     Dynamic Shapes
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
    <label for="toctree-checkbox-23">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="dynamic-shapes/dynamic-shapes-not-applicable.html">
       When Dynamic Shapes API is Not Applicable
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="current reference internal" href="#">
     Automatic Device Selection
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
    <label for="toctree-checkbox-24">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="automatic-device-selection/debugging-auto-device.html">
       Debugging Auto-Device Plugin
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi-device-execution-mode.html">
     Running on Multiple Devices Simultaneously
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="heterogeneous-execution-mode.html">
     Heterogeneous execution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="performance-hints.html">
     High-level Performance Hints
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="automatic-batching.html">
     Automatic Batching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="stateful-models.html">
     Stateful models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../openvino-deployment-guide.html">
   Deploying Your Applications with OpenVINO™
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
  <label for="toctree-checkbox-25">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-deployment-guide/deployment-manager-tool.html">
     Deploying Your Application with Deployment Manager
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-deployment-guide/local-distribution.html">
     Libraries for Local Distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../compile-tool.html">
   Compile Tool
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  THE Ecosystem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../openvino-security-add-on.html">
   OpenVINO™ Security Add-on
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../dl-workbench-overview.html">
   OpenVINO™ Deep Learning Workbench Overview
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
  <label for="toctree-checkbox-26">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../dl-workbench-overview/dl-workbench-install.html">
     Installation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
    <label for="toctree-checkbox-27">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-install/dl-workbench-install-prerequisites.html">
       Prerequisites
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-install/run-dl-workbench-locally.html">
       Run the DL Workbench Locally
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
      <label for="toctree-checkbox-28">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/dl-workbench-install/run-dl-workbench-locally/dl-workbench-cofigurations.html">
         Advanced DL Workbench Configurations
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/dl-workbench-install/run-dl-workbench-locally/dl-workbench-docker.html">
         Work with Docker Container
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-install/run-dl-workbench-in-devcloud.html">
       Run the DL Workbench in the Intel® DevCloud for the Edge
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../dl-workbench-overview/dl-workbench-get-started.html">
     Get Started
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
    <label for="toctree-checkbox-29">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-get-started/dl-workbench-import-model.html">
       Import Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-get-started/dl-workbench-create-project.html">
       Create Project
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-get-started/dl-workbench-resources.html">
       Educational Resources about DL Workbench
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
      <label for="toctree-checkbox-30">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/dl-workbench-get-started/dl-workbench-resources/dl-workbench-key-concepts.html">
         DL Workbench Key Concepts
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../dl-workbench-overview/dl-workbench-tutorials.html">
     Tutorials
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
    <label for="toctree-checkbox-31">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_Import_YOLO.html">
       Object Detection Model (YOLOv4)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Measure_Accuracy_Object_detection.html">
       Object Detection Model (SSD_mobilenet)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_Classification.html">
       Classification Model (mobilenet)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Measure_Accuracy_Classification.html">
       Classification Model (squeezenet)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_Instance_Segmentation.html">
       Instance Segmentation Model (mask R-cnn)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_Semantic_Segmentation.html">
       Semantic Segmentation Model (deeplab)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_Style_Transfer.html">
       Style Transfer Model (fast-nst-onnx)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_NLP.html">
       NLP Model (BERT)
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../dl-workbench-overview/dl-workbench-user-guide.html">
     User Guide
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
    <label for="toctree-checkbox-32">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Select_Models.html">
       Obtain Models
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/>
      <label for="toctree-checkbox-33">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_OMZ_Models.html">
         Import Open Model Zoo Models
        </a>
       </li>
       <li class="toctree-l4 has-children">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Original_Model_Import.html">
         Import Original Model
        </a>
        <input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/>
        <label for="toctree-checkbox-34">
         <i class="fas fa-chevron-down">
         </i>
        </label>
        <ul>
         <li class="toctree-l5">
          <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Tutorial_Import_Original.html">
           Import Original Model Recommendations
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Generate_Datasets.html">
       Obtain Datasets
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/>
      <label for="toctree-checkbox-35">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4 has-children">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Dataset_Types.html">
         Dataset Types
        </a>
        <input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/>
        <label for="toctree-checkbox-36">
         <i class="fas fa-chevron-down">
         </i>
        </label>
        <ul>
         <li class="toctree-l5">
          <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Download_and_Cut_Datasets.html">
           Cut Datasets
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Select_Environment.html">
       Select Environment
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/>
      <label for="toctree-checkbox-37">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4 has-children">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Remote_Profiling.html">
         Work with Remote Targets
        </a>
        <input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/>
        <label for="toctree-checkbox-38">
         <i class="fas fa-chevron-down">
         </i>
        </label>
        <ul>
         <li class="toctree-l5">
          <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Profile_on_Remote_Machine.html">
           Profile on Remote Machine
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Setup_Remote_Target.html">
           Set Up Remote Target
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Add_Remote_Target.html">
           Register Remote Target in DL Workbench
          </a>
         </li>
         <li class="toctree-l5">
          <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Remote_Machines.html">
           Manipulate Remote Machines
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Int_8_Quantization.html">
       Optimize Model Performance
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Run_Inference.html">
       Explore Inference Configurations
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/>
      <label for="toctree-checkbox-39">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Run_Single_Inference.html">
         Run Inference
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_View_Inference_Results.html">
         View Inference Results
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Compare_Performance_between_Two_Versions_of_Models.html">
         Compare Performance between Two Versions of a Model
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Visualize_Model.html">
         Visualize Model
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Visualize_Accuracy.html">
       Visualize Model Output
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Measure_Accuracy.html">
       Create Accuracy Report
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/>
      <label for="toctree-checkbox-40">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Accuracy_Configuration.html">
         Accuracy Configuration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Configure_Accuracy_Settings.html">
         Set Accuracy Configuration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Accuracy_Report_Results.html">
         Interpret Accuracy Report Results
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Deployment_Package.html">
       Create Deployment Package
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/>
      <label for="toctree-checkbox-41">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html">
         Deploy and Integrate Performance Criteria into Application
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Export_Project.html">
       Export Project
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Jupyter_Notebooks_CLI.html">
       Learn OpenVINO in DL Workbench
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/>
      <label for="toctree-checkbox-42">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Jupyter_Notebooks.html">
         Learn Model Inference with OpenVINO™ API in JupyterLab Environment
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Persist_Database.html">
       Restore DL Workbench State
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_security_Workbench.html">
       Run DL Workbench Securely
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/>
      <label for="toctree-checkbox-43">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Authentication.html">
         Enable Authentication in DL Workbench
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../dl-workbench-overview/_workbench-files-to-migrate/workbench_docs_Workbench_DG_Configure_TLS.html">
         Configure Transport Layer Security (TLS)
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../dl-workbench-overview/dl-workbench-troubleshooting.html">
     Troubleshooting
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/>
    <label for="toctree-checkbox-44">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../dl-workbench-overview/dl-workbench-troubleshooting/dl-workbench-devcloud-troubleshooting.html">
       Troubleshooting for DL Workbench in the Intel® DevCloud for the Edge
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Media Processing and Computer Vision Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intel-deep-learning-streamer.html">
   Intel® Deep Learning Streamer
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../opencv-graph-api.html">
   Introduction to OpenCV Graph API
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/>
  <label for="toctree-checkbox-45">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../opencv-graph-api/graph-api-kernel.html">
     Graph API Kernel API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../opencv-graph-api/face-beautification-algorithm.html">
     Implementing a Face Beautification Algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../opencv-graph-api/face-analytics-pipeline.html">
     Building a Face Analytics Pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://docs.opencv.org/master/">
   OpenCV Developer Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://software.intel.com/en-us/openclsdk-devguide">
   OpenCL™ Developer Guide
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  OpenVINO Extensibility
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../openvino-extensibility-mechanism.html">
   OpenVINO Extensibility Mechanism
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/>
  <label for="toctree-checkbox-46">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-extensibility-mechanism/custom-openvino-operations.html">
     Custom OpenVINO™ Operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-extensibility-mechanism/frontend-extensions.html">
     Frontend Extensions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-extensibility-mechanism/custom-operations_for_gpu.html">
     How to Implement Custom Operations for GPU
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-extensibility-mechanism/custom-operations_for_vpu.html">
     How to Implement Custom Operations for VPU (Intel® Neural Compute Stick 2)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../openvino-extensibility-mechanism/model_optimizer_extensibility.html">
     Model Optimizer Extensibility
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/>
    <label for="toctree-checkbox-47">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../openvino-extensibility-mechanism/model_optimizer_extensibility/extending-model-optimizer-with-caffe-python-layers.html">
       Extending Model Optimizer with Caffe Python Layers
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../openvino-transformation-api.html">
   Overview of Transformations API
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/>
  <label for="toctree-checkbox-48">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-transformation-api/transformation-api-model-pass.html">
     OpenVINO Model Pass
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-transformation-api/transformation-api-matcher-pass.html">
     OpenVINO Matcher Pass
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-transformation-api/transformation-api-graph-rewrite-pass.html">
     OpenVINO Graph Rewrite Pass
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../openvino-plugin-developer-guide.html">
   Overview of Inference Engine Plugin Library
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/>
  <label for="toctree-checkbox-49">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/openvino-custom-plugins.html">
     Plugin
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/executable-network-class-in-custom-plugins.html">
     Executable Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/synchronous-inference-request.html">
     Synchronous Inference Request
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/asynchronous-inference-request.html">
     Asynchronous Inference Request
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/building-custom-plugins-with-cmake.html">
     Build Plugin Using CMake
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/testing-custom-openvino-plugins.html">
     Plugin Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../openvino-plugin-developer-guide/quantized_network_support.html">
     Quantized networks compute and restrictions
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations.html">
     OpenVINO™ Low Precision Transformations
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/>
    <label for="toctree-checkbox-50">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes.html">
       Low-Precision Transformation Attributes
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/>
      <label for="toctree-checkbox-51">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes/avgpoolprecisionpreserved.html">
         AvgPoolPrecisionPreserved attribute
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes/intervalsalignment.html">
         IntervalsAlignment attribute
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes/precisionpreserved.html">
         PrecisionPreserved attribute
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes/precisions.html">
         Precisions attribute
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes/quantizationalignment.html">
         QuantizationAlignment attribute
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/low-precision-transformation-attributes/quantizationgranularity.html">
         QuantizationGranularity attribute
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/prerequisites-transformations.html">
       Step 1. Prerequisites Transformations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/markup-transformations.html">
       Step 2. Markup Transformations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/main-transformations.html">
       Step 3. Main Transformations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../openvino-plugin-developer-guide/low-precision-transformations/cleanup-transformations.html">
       Step 4. Cleanup Transformations
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../openvino-plugin-developer-guide/custom-plugin-api-reference.html">
     Plugin API Reference
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/>
    <label for="toctree-checkbox-52">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="simple">
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Use OpenVINO™ Toolkit Securely
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../openvino-security-introduction.html">
   Introduction to OpenVINO™ Security
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../openvino-dl-workbench-security.html">
   Deep Learning Workbench Security
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../using-encrypted-models-with-openvino.html">
   Using Encrypted Models with OpenVINO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../openvino-security-add-on.html">
   OpenVINO™ Security Add-on
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-auto-works">
   How AUTO Works
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-auto">
   Using AUTO
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#device-candidates-and-priority">
     Device Candidates and Priority
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#checking-available-devices">
       Checking Available Devices
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#excluding-devices-from-device-candidate-list">
       Excluding Devices from Device Candidate List
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-hints-for-auto">
     Performance Hints for AUTO
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#throughput">
       THROUGHPUT
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#latency">
       LATENCY
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cumulative-throughput">
       CUMULATIVE_THROUGHPUT
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#code-examples">
       Code Examples
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#disabling-auto-batching-for-throughput-and-cumulative-throughput">
       Disabling Auto-Batching for THROUGHPUT and CUMULATIVE_THROUGHPUT
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configuring-model-priority">
     Configuring Model Priority
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#configuring-individual-devices-and-creating-the-auto-device-plugin-on-top">
   Configuring Individual Devices and Creating the Auto-Device plugin on Top
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-auto-with-openvino-samples-and-benchmark-app">
   Using AUTO with OpenVINO Samples and Benchmark app
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-resources">
   Additional Resources
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">

<div class="tocsection editthispage">
    <a href="None">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

            
                <div>
                  
  <span class="target" id="deploy-infer-auto-plugin"><span id="index-0"></span></span><section id="automatic-device-selection">
<h1>Automatic Device Selection<a class="headerlink" href="#automatic-device-selection" title="Permalink to this headline">¶</a></h1>
<p><span class="target" id="deploy-infer-auto-plugin-1md-openvino-docs-ov-runtime-ug-auto-device-selection"></span></p>
<div class="toctree-wrapper compound">
</div>
<p>This article introduces how Automatic Device Selection works and how to use it for inference.</p>
<section id="how-auto-works">
<h2>How AUTO Works<a class="headerlink" href="#how-auto-works" title="Permalink to this headline">¶</a></h2>
<p>The Automatic Device Selection mode, or AUTO for short, uses a “virtual” or a “proxy”
device, which does not bind to a specific type of hardware, but rather selects the
processing unit for inference automatically. It detects available devices, picks the
one best-suited for the task, and configures its optimization settings. This way,
you can write the application once and deploy it anywhere.</p>
<p>The selection also depends on your performance requirements, defined by the “hints”
configuration API, as well as device priority list limitations, if you choose to
exclude some hardware from the process.</p>
<p>The logic behind the choice is as follows:</p>
<ol class="arabic simple">
<li><p>Check what supported devices are available.</p></li>
<li><p>Check precisions of the input model (for detailed information on precisions read more on the <code class="docutils literal notranslate"><a class="reference internal" href="../../../api/groups/groupov_runtime_cpp_prop_api.html#doxid-group-ov-runtime-cpp-prop-api-1gadb13d62787fc4485733329f044987294"><span class="std std-ref"><span class="pre">ov::device::capabilities</span></span></a></code>)</p></li>
<li><p>Select the highest-priority device capable of supporting the given model, as listed in the table below.</p></li>
<li><p>If model’s precision is FP32 but there is no device capable of supporting it, offload the model to a device supporting FP16.</p></li>
</ol>
<table class="table">
<colgroup>
<col style="width: 10%" />
<col style="width: 53%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Device
Priority</p></th>
<th class="head"><div class="line-block">
<div class="line">Supported</div>
<div class="line">Device</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Supported</div>
<div class="line">model precision</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><div class="line-block">
<div class="line">dGPU</div>
<div class="line">(e.g. Intel® Iris® Xe MAX)</div>
</div>
</td>
<td><p>FP32, FP16, INT8, BIN</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><div class="line-block">
<div class="line">iGPU</div>
<div class="line">(e.g. Intel® UHD Graphics 620 (iGPU))</div>
</div>
</td>
<td><p>FP32, FP16, BIN</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><div class="line-block">
<div class="line">Intel® Movidius™ Myriad™ X VPU</div>
<div class="line">(e.g. Intel® Neural Compute Stick 2 (Intel® NCS2))</div>
</div>
</td>
<td><p>FP16</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><div class="line-block">
<div class="line">Intel® CPU</div>
<div class="line">(e.g. Intel® Core™ i7-1165G7)</div>
</div>
</td>
<td><p>FP32, FP16, INT8, BIN</p></td>
</tr>
</tbody>
</table>
<p>To put it simply, when loading the model to the first device on the list fails, AUTO
will try to load it to the next device in line, until one of them succeeds. What is
important, <strong>AUTO always starts inference with the CPU of the system</strong>, as it provides
very low latency and can start inference with no additional delays. While the CPU is
performing inference, AUTO continues to load the model to the device best suited for
the purpose and transfers the task to it when ready. This way, the devices which are
much slower in compiling models, GPU being the best example, do not impede inference
at its initial stages. For example, if you use a CPU and a GPU, the first-inference
latency of AUTO will be better than that of using GPU alone.</p>
<p>Note that if you choose to exclude CPU from the priority list, it will be unable to
support the initial model compilation stage.</p>
<img alt="../../../_images/autoplugin_accelerate.png" src="../../../_images/autoplugin_accelerate.png" />
<p>This mechanism can be easily observed in the
<a class="reference external" href="#using-auto-with-openvino-samples-and-benchmark-app">Using AUTO with Benchmark app sample</a>
section, showing how the first-inference latency (the time it takes to compile the
model and perform the first inference) is reduced when using AUTO. For example:</p>
<div class="highlight"><div class="highlight"><pre><span></span><span class="n">benchmark_app</span> <span class="o">-</span><span class="n">m</span> <span class="p">..</span><span class="o">/</span><span class="k">public</span><span class="o">/</span><span class="n">alexnet</span><span class="o">/</span><span class="n">FP32</span><span class="o">/</span><span class="n">alexnet</span><span class="p">.</span><span class="n">xml</span> <span class="o">-</span><span class="n">d</span> <span class="n">GPU</span> <span class="o">-</span><span class="n">niter</span> <span class="mi">128</span></pre></div></div><div class="highlight"><div class="highlight"><pre><span></span><span class="n">benchmark_app</span> <span class="o">-</span><span class="n">m</span> <span class="p">..</span><span class="o">/</span><span class="k">public</span><span class="o">/</span><span class="n">alexnet</span><span class="o">/</span><span class="n">FP32</span><span class="o">/</span><span class="n">alexnet</span><span class="p">.</span><span class="n">xml</span> <span class="o">-</span><span class="n">d</span> <span class="n">AUTO</span> <span class="o">-</span><span class="n">niter</span> <span class="mi">128</span></pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The longer the process runs, the closer realtime performance will be to that of
the best-suited device.</p>
</div>
</section>
<section id="using-auto">
<h2>Using AUTO<a class="headerlink" href="#using-auto" title="Permalink to this headline">¶</a></h2>
<p>Following the OpenVINO™ naming convention, the Automatic Device Selection mode is
assigned the label of “AUTO.” It may be defined with no additional parameters, resulting
in defaults being used, or configured further with the following setup options:</p>
<table class="table">
<colgroup>
<col style="width: 31%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">Property</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Values and Description</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">&lt;device candidate list&gt;</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><strong>Values</strong>:</div>
<div class="line-block">
<div class="line">empty</div>
<div class="line"><cite>AUTO</cite></div>
<div class="line"><cite>AUTO: &lt;device names&gt;</cite> (comma-separated, no spaces)</div>
<div class="line"><br /></div>
</div>
<div class="line">Lists the devices available for selection.</div>
<div class="line">The device sequence will be taken as priority from high to low.</div>
<div class="line">If not specified, <cite>AUTO</cite> will be used as default,</div>
<div class="line">and all devices will be “viewed” as candidates.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line"><cite>ov::device:priorities</cite></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><strong>Values</strong>:</div>
<div class="line-block">
<div class="line"><cite>&lt;device names&gt;</cite> (comma-separated, no spaces)</div>
<div class="line"><br /></div>
</div>
<div class="line">Specifies the devices for AUTO to select.</div>
<div class="line">The device sequence will be taken as priority from high to low.</div>
<div class="line">This configuration is optional.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line"><cite>ov::hint::performance_mode</cite></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><strong>Values</strong>:</div>
<div class="line-block">
<div class="line"><cite>ov::hint::PerformanceMode::LATENCY</cite></div>
<div class="line"><cite>ov::hint::PerformanceMode::THROUGHPUT</cite></div>
<div class="line"><cite>ov::hint::PerformanceMode::CUMULATIVE_THROUGHPUT</cite></div>
<div class="line"><br /></div>
</div>
<div class="line">Specifies the performance option preferred by the application.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line"><cite>ov::hint::model_priority</cite></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line"><strong>Values</strong>:</div>
<div class="line-block">
<div class="line"><cite>ov::hint::Priority::HIGH</cite></div>
<div class="line"><cite>ov::hint::Priority::MEDIUM</cite></div>
<div class="line"><cite>ov::hint::Priority::LOW</cite></div>
<div class="line"><br /></div>
</div>
<div class="line">Indicates the priority for a model.</div>
<div class="line"><strong>IMPORTANT: This property is not fully supported yet.</strong></div>
</div>
</td>
</tr>
</tbody>
</table>
<p>Inference with AUTO is configured similarly to when device plugins are used: you
compile the model on the plugin with configuration and execute inference.</p>
<section id="device-candidates-and-priority">
<h3>Device Candidates and Priority<a class="headerlink" href="#device-candidates-and-priority" title="Permalink to this headline">¶</a></h3>
<p>The device candidate list enables you to customize the priority and limit the choice
of devices available to AUTO.</p>
<ul class="simple">
<li><p>If &lt;device candidate list&gt; is not specified, AUTO assumes all the devices present in the system can be used.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">AUTO</span></code> without any device names is specified, AUTO assumes all the devices present in the system can be used, and will load the network to all devices and run inference based on their default priorities, from high to low.</p></li>
</ul>
<p>To specify the priority of devices, enter the device names in the priority order
(from high to low) in <code class="docutils literal notranslate"><span class="pre">AUTO:</span> <span class="pre">&lt;device</span> <span class="pre">names&gt;</span></code>, or use the
<code class="docutils literal notranslate"><a class="reference internal" href="../../../api/groups/namespaceov_1_1device.html#doxid-namespaceov-1-1device"><span class="std std-ref"><span class="pre">ov::device</span></span></a><span class="pre">:priorities</span></code> property.</p>
<p>See the following code for using AUTO and specifying devices:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">C++</label><div class="tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ov</span><span class="o">::</span><span class="n">Core</span> <span class="n">core</span><span class="p">;</span>

<span class="c1">// Read a network in IR, PaddlePaddle, or ONNX format:</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ov</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">read_model</span><span class="p">(</span><span class="s">&quot;sample.xml&quot;</span><span class="p">);</span>

<span class="c1">// compile a model on AUTO using the default list of device candidates.</span>
<span class="c1">// The following lines are equivalent:</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">model0</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">model1</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">);</span>

<span class="c1">// Optional</span>
<span class="c1">// You can also specify the devices to be used by AUTO.</span>
<span class="c1">// The following lines are equivalent:</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">model3</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO:GPU,CPU&quot;</span><span class="p">);</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">model4</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">ov</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">priorities</span><span class="p">(</span><span class="s">&quot;GPU,CPU&quot;</span><span class="p">));</span>

<span class="c1">//Optional</span>
<span class="c1">// the AUTO plugin is pre-configured (globally) with the explicit option:</span>
<span class="n">core</span><span class="p">.</span><span class="n">set_property</span><span class="p">(</span><span class="s">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">ov</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">priorities</span><span class="p">(</span><span class="s">&quot;GPU,CPU&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Python</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">core</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>

    <span class="c1"># Read a network in IR, PaddlePaddle, or ONNX format:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

    <span class="c1">#  compile a model on AUTO using the default list of device candidates.</span>
    <span class="c1">#  The following lines are equivalent:</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">)</span>

    <span class="c1"># Optional</span>
    <span class="c1"># You can also specify the devices to be used by AUTO.</span>
    <span class="c1"># The following lines are equivalent:</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO:GPU,CPU&quot;</span><span class="p">)</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MULTI_DEVICE_PRIORITIES&quot;</span><span class="p">:</span> <span class="s2">&quot;GPU,CPU&quot;</span><span class="p">})</span>

    <span class="c1"># Optional</span>
    <span class="c1"># the AUTO plugin is pre-configured (globally) with the explicit option:</span>
    <span class="n">core</span><span class="o">.</span><span class="n">set_property</span><span class="p">(</span><span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">properties</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MULTI_DEVICE_PRIORITIES&quot;</span><span class="p">:</span><span class="s2">&quot;GPU,CPU&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>Note that OpenVINO Runtime lets you use “GPU” as an alias for “GPU.0” in function
calls. More details on enumerating devices can be found in
<a class="reference internal" href="working-with-devices.html#deploy-infer-working-with-devices"><span class="std std-ref">Working with devices</span></a>.</p>
<section id="checking-available-devices">
<h4>Checking Available Devices<a class="headerlink" href="#checking-available-devices" title="Permalink to this headline">¶</a></h4>
<p>To check what devices are present in the system, you can use Device API, as listed
below. For information on how to use it, see
<a class="reference internal" href="working-with-devices/query-device-properties.html#deploy-infer-query-device-properties"><span class="std std-ref">Query device properties and configuration</span></a>.</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">C++</label><div class="tab-content docutils">
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>ov::runtime::Core::get_available_devices<span class="o">()</span>
</pre></div>
</div>
<p>See the Hello Query Device C++ Sample for reference.</p>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">Python</label><div class="tab-content docutils">
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>openvino.runtime.Core.available_devices
</pre></div>
</div>
<p>See the Hello Query Device Python Sample for reference.</p>
</div>
</div>
</section>
<section id="excluding-devices-from-device-candidate-list">
<h4>Excluding Devices from Device Candidate List<a class="headerlink" href="#excluding-devices-from-device-candidate-list" title="Permalink to this headline">¶</a></h4>
<p>You can also exclude hardware devices from AUTO, for example, to reserve CPU for
other jobs. AUTO will not use the device for inference then. To do that, add a minus
sign (-) before CPU in <code class="docutils literal notranslate"><span class="pre">AUTO:</span> <span class="pre">&lt;device</span> <span class="pre">names&gt;</span></code>, as in the following example:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--2-input--1" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--1">C++</label><div class="tab-content docutils">
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>ov::CompiledModel <span class="nv">compiled_model</span> <span class="o">=</span> core.compile_model<span class="o">(</span>model, <span class="s2">&quot;AUTO:-CPU&quot;</span><span class="o">)</span><span class="p">;</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--2-input--2" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--2">Python</label><div class="tab-content docutils">
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">compiled_model</span> <span class="o">=</span> core.compile_model<span class="o">(</span><span class="nv">model</span><span class="o">=</span>model, <span class="nv">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO:-CPU&quot;</span><span class="o">)</span>
</pre></div>
</div>
</div>
</div>
<p>AUTO will then query all available devices and remove CPU from the candidate list.</p>
<p>Note that if you choose to exclude CPU from device candidate list, CPU will not be
able to support the initial model compilation stage. See more information in
<a class="reference external" href="#how-auto-works">How AUTO Works</a>.</p>
</section>
</section>
<section id="performance-hints-for-auto">
<h3>Performance Hints for AUTO<a class="headerlink" href="#performance-hints-for-auto" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><a class="reference internal" href="../../../api/groups/groupov_runtime_cpp_prop_api.html#doxid-group-ov-runtime-cpp-prop-api-1ga2691fe27acc8aa1d1700ad40b6da3ba2"><span class="std std-ref"><span class="pre">ov::hint::performance_mode</span></span></a></code>
property enables you to specify a performance option for AUTO to be more efficient
for particular use cases.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, the <code class="docutils literal notranslate"><a class="reference internal" href="../../../api/groups/namespaceov_1_1hint.html#doxid-namespaceov-1-1hint"><span class="std std-ref"><span class="pre">ov::hint</span></span></a></code> property is
supported by CPU and GPU devices only.</p>
</div>
<section id="throughput">
<h4>THROUGHPUT<a class="headerlink" href="#throughput" title="Permalink to this headline">¶</a></h4>
<p>This option prioritizes high throughput, balancing between latency and power. It is
best suited for tasks involving multiple jobs, such as inference of video feeds or
large numbers of images.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If no performance hint is set explicitly, AUTO will set THROUGHPUT for devices
that have not set <code class="docutils literal notranslate"><span class="pre">ov::device::properties</span></code>. For example, if you have both a CPU
and a GPU in the system, this command <code class="docutils literal notranslate"><span class="pre">core.compile_model(&quot;AUTO&quot;,</span> <span class="pre">ov::device::properties(&quot;CPU&quot;,</span> <span class="pre">ov::enable_profiling(true)))</span></code>
will set THROUGHPUT for the GPU only. No hint will be set for the CPU although
it’s the selected device.</p>
</div>
</section>
<section id="latency">
<h4>LATENCY<a class="headerlink" href="#latency" title="Permalink to this headline">¶</a></h4>
<p>This option prioritizes low latency, providing short response time for each inference
job. It performs best for tasks where inference is required for a single input image,
e.g. a medical analysis of an ultrasound scan image. It also fits the tasks of real-time
or nearly real-time applications, such as an industrial robot’s response to actions
in its environment or obstacle avoidance for autonomous vehicles.</p>
</section>
<section id="cumulative-throughput">
<span id="id1"></span><h4>CUMULATIVE_THROUGHPUT<a class="headerlink" href="#cumulative-throughput" title="Permalink to this headline">¶</a></h4>
<p>While <code class="docutils literal notranslate"><span class="pre">LATENCY</span></code> and <code class="docutils literal notranslate"><span class="pre">THROUGHPUT</span></code> can select one target device with your preferred
performance option, the <code class="docutils literal notranslate"><span class="pre">CUMULATIVE_THROUGHPUT</span></code> option enables running inference on
multiple devices for higher throughput. With <code class="docutils literal notranslate"><span class="pre">CUMULATIVE_THROUGHPUT</span></code>, AUTO loads
the network model to all available devices in the candidate list, and then runs
inference on them based on the default or specified priority.</p>
<p><code class="docutils literal notranslate"><span class="pre">CUMULATIVE_THROUGHPUT</span></code> has similar behavior as
<a class="reference internal" href="multi-device-execution-mode.html#deploy-infer-multi-plugin"><span class="std std-ref">the Multi-Device execution mode (MULTI)</span></a>.
The only difference is that <code class="docutils literal notranslate"><span class="pre">CUMULATIVE_THROUGHPUT</span></code> uses the devices specified by AUTO,
which means that it’s not mandatory to add devices manually, while with MULTI, you
need to specify the devices before inference.</p>
<p>With the <code class="docutils literal notranslate"><span class="pre">CUMULATIVE_THROUGHPUT</span></code> option:</p>
<ul>
<li><p>If <code class="docutils literal notranslate"><span class="pre">AUTO</span></code> without any device names is specified, and the system has more than one GPU devices, AUTO will remove CPU from the device candidate list to keep GPU running at full capacity.</p></li>
<li><p>If device priority is specified, AUTO will run inference requests on devices based on the priority. In the following example, AUTO will always try to use GPU first, and then use CPU if GPU is busy:</p>
<div class="highlight"><div class="highlight"><pre><span></span><span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO:GPU,CPU&quot;</span><span class="p">,</span> <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">performance_mode</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">PerformanceMode</span><span class="o">::</span><span class="n">CUMULATIVE_THROUGHPUT</span><span class="p">));</span></pre></div></div></li>
</ul>
</section>
<section id="code-examples">
<h4>Code Examples<a class="headerlink" href="#code-examples" title="Permalink to this headline">¶</a></h4>
<p>To enable performance hints for your application, use the following code:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--3-input--1" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--1">C++</label><div class="tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ov</span><span class="o">::</span><span class="n">Core</span> <span class="n">core</span><span class="p">;</span>

<span class="c1">// Read a network in IR, PaddlePaddle, or ONNX format:</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ov</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">read_model</span><span class="p">(</span><span class="s">&quot;sample.xml&quot;</span><span class="p">);</span>

<span class="c1">// Compile a model on AUTO with Performance Hint enabled:</span>
<span class="c1">// To use the “THROUGHPUT” option:</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">performance_mode</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">PerformanceMode</span><span class="o">::</span><span class="n">THROUGHPUT</span><span class="p">));</span>
<span class="c1">// To use the “LATENCY” option:</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_mode2</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">performance_mode</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">PerformanceMode</span><span class="o">::</span><span class="n">LATENCY</span><span class="p">));</span>
<span class="c1">// To use the “CUMULATIVE_THROUGHPUT” option:</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_mode3</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">performance_mode</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">PerformanceMode</span><span class="o">::</span><span class="n">CUMULATIVE_THROUGHPUT</span><span class="p">));</span>    
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--3-input--2" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--2">Python</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">core</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
    <span class="c1"># Read a network in IR, PaddlePaddle, or ONNX format:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># Compile a model on AUTO with Performance Hints enabled:</span>
    <span class="c1"># To use the “THROUGHPUT” mode:</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;PERFORMANCE_HINT&quot;</span><span class="p">:</span><span class="s2">&quot;THROUGHPUT&quot;</span><span class="p">})</span>
    <span class="c1"># To use the “LATENCY” mode:</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;PERFORMANCE_HINT&quot;</span><span class="p">:</span><span class="s2">&quot;LATENCY&quot;</span><span class="p">})</span>
    <span class="c1"># To use the “CUMULATIVE_THROUGHPUT” mode:</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;PERFORMANCE_HINT&quot;</span><span class="p">:</span><span class="s2">&quot;CUMULATIVE_THROUGHPUT&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="disabling-auto-batching-for-throughput-and-cumulative-throughput">
<h4>Disabling Auto-Batching for THROUGHPUT and CUMULATIVE_THROUGHPUT<a class="headerlink" href="#disabling-auto-batching-for-throughput-and-cumulative-throughput" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">ov::hint::PerformanceMode::THROUGHPUT</span></code> mode and the
<code class="docutils literal notranslate"><span class="pre">ov::hint::PerformanceMode::CUMULATIVE_THROUGHPUT</span></code> mode will trigger Auto-Batching
(for example, for the GPU device) by default. You can disable it by setting
<code class="docutils literal notranslate"><span class="pre">ov::hint::allow_auto_batching(false)</span></code>, or change the default timeout value to a
large number, e.g. <code class="docutils literal notranslate"><span class="pre">ov::auto_batch_timeout(1000)</span></code>.
See <a class="reference internal" href="automatic-batching.html#deploy-infer-automatic-batching"><span class="std std-ref">Automatic Batching</span></a>
for more details.</p>
</section>
</section>
<section id="configuring-model-priority">
<h3>Configuring Model Priority<a class="headerlink" href="#configuring-model-priority" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><a class="reference internal" href="../../../api/groups/groupov_runtime_cpp_prop_api.html#doxid-group-ov-runtime-cpp-prop-api-1ga3663a3976ff7c4bdc3ccdb9ce44945ce"><span class="std std-ref"><span class="pre">ov::hint::model_priority</span></span></a></code>
property enables you to control the priorities of models in the Auto-Device plugin.
A high-priority model will be loaded to a supported high-priority device. A lower-priority
model will not be loaded to a device that is occupied by a higher-priority model.</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--4-input--1" name="tab-set--4" type="radio"><label class="tab-label" for="tab-set--4-input--1">C++</label><div class="tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Example 1</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model0</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">model_priority</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">Priority</span><span class="o">::</span><span class="n">HIGH</span><span class="p">));</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model1</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">model_priority</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">Priority</span><span class="o">::</span><span class="n">MEDIUM</span><span class="p">));</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model2</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">model_priority</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">Priority</span><span class="o">::</span><span class="n">LOW</span><span class="p">));</span>
<span class="cm">/************</span>
<span class="cm">  Assume that all the devices (CPU, GPU, and MYRIAD) can support all the networks.</span>
<span class="cm">  Result: compiled_model0 will use GPU, compiled_model1 will use MYRIAD, compiled_model2 will use CPU.</span>
<span class="cm"> ************/</span>

<span class="c1">// Example 2</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model3</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">model_priority</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">Priority</span><span class="o">::</span><span class="n">LOW</span><span class="p">));</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model4</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">model_priority</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">Priority</span><span class="o">::</span><span class="n">MEDIUM</span><span class="p">));</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model5</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">model_priority</span><span class="p">(</span><span class="n">ov</span><span class="o">::</span><span class="n">hint</span><span class="o">::</span><span class="n">Priority</span><span class="o">::</span><span class="n">LOW</span><span class="p">));</span>
<span class="cm">/************</span>
<span class="cm">  Assume that all the devices (CPU, GPU, and MYRIAD) can support all the networks.</span>
<span class="cm">  Result: compiled_model3 will use GPU, compiled_model4 will use GPU, compiled_model5 will use MYRIAD.</span>
<span class="cm"> ************/</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--4-input--2" name="tab-set--4" type="radio"><label class="tab-label" for="tab-set--4-input--2">Python</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">core</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

    <span class="c1"># Example 1</span>
    <span class="n">compiled_model0</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MODEL_PRIORITY&quot;</span><span class="p">:</span><span class="s2">&quot;HIGH&quot;</span><span class="p">})</span>
    <span class="n">compiled_model1</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MODEL_PRIORITY&quot;</span><span class="p">:</span><span class="s2">&quot;MEDIUM&quot;</span><span class="p">})</span>
    <span class="n">compiled_model2</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MODEL_PRIORITY&quot;</span><span class="p">:</span><span class="s2">&quot;LOW&quot;</span><span class="p">})</span>
    <span class="c1"># Assume that all the devices (CPU, GPU, and MYRIAD) can support all the networks.</span>
    <span class="c1"># Result: compiled_model0 will use GPU, compiled_model1 will use MYRIAD, compiled_model2 will use CPU.</span>

    <span class="c1"># Example 2</span>
    <span class="n">compiled_model3</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MODEL_PRIORITY&quot;</span><span class="p">:</span><span class="s2">&quot;HIGH&quot;</span><span class="p">})</span>
    <span class="n">compiled_model4</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MODEL_PRIORITY&quot;</span><span class="p">:</span><span class="s2">&quot;MEDIUM&quot;</span><span class="p">})</span>
    <span class="n">compiled_model5</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;MODEL_PRIORITY&quot;</span><span class="p">:</span><span class="s2">&quot;LOW&quot;</span><span class="p">})</span>
    <span class="c1"># Assume that all the devices (CPU, GPU, and MYRIAD) can support all the networks.</span>
    <span class="c1"># Result: compiled_model3 will use GPU, compiled_model4 will use GPU, compiled_model5 will use MYRIAD.</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="configuring-individual-devices-and-creating-the-auto-device-plugin-on-top">
<h2>Configuring Individual Devices and Creating the Auto-Device plugin on Top<a class="headerlink" href="#configuring-individual-devices-and-creating-the-auto-device-plugin-on-top" title="Permalink to this headline">¶</a></h2>
<p>Although the methods described above are currently the preferred way to execute
inference with AUTO, the following steps can be also used as an alternative. It is
currently available as a legacy feature and used if the device candidate list includes
Myriad devices, incapable of utilizing the Performance Hints option.</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--5-input--1" name="tab-set--5" type="radio"><label class="tab-label" for="tab-set--5-input--1">C++</label><div class="tab-content docutils">
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ov</span><span class="o">::</span><span class="n">Core</span> <span class="n">core</span><span class="p">;</span>

<span class="c1">// Read a network in IR, PaddlePaddle, or ONNX format:</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ov</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">read_model</span><span class="p">(</span><span class="s">&quot;sample.xml&quot;</span><span class="p">);</span>

<span class="c1">// Configure  CPU and the MYRIAD devices when compiled model</span>
<span class="n">ov</span><span class="o">::</span><span class="n">CompiledModel</span> <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">&quot;AUTO&quot;</span><span class="p">,</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">properties</span><span class="p">(</span><span class="s">&quot;CPU&quot;</span><span class="p">,</span> <span class="n">cpu_config</span><span class="p">),</span>
    <span class="n">ov</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">properties</span><span class="p">(</span><span class="s">&quot;MYRIAD&quot;</span><span class="p">,</span> <span class="n">myriad_config</span><span class="p">));</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--5-input--2" name="tab-set--5" type="radio"><label class="tab-label" for="tab-set--5-input--2">Python</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">core</span> <span class="o">=</span> <span class="n">Core</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">read_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="n">core</span><span class="o">.</span><span class="n">set_property</span><span class="p">(</span><span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="n">properties</span><span class="o">=</span><span class="p">{})</span>
    <span class="n">core</span><span class="o">.</span><span class="n">set_property</span><span class="p">(</span><span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;MYRIAD&quot;</span><span class="p">,</span> <span class="n">properties</span><span class="o">=</span><span class="p">{})</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
    <span class="n">compiled_model</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">device_name</span><span class="o">=</span><span class="s2">&quot;AUTO&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-auto-with-openvino-samples-and-benchmark-app">
<h2>Using AUTO with OpenVINO Samples and Benchmark app<a class="headerlink" href="#using-auto-with-openvino-samples-and-benchmark-app" title="Permalink to this headline">¶</a></h2>
<p>To see how the Auto-Device plugin is used in practice and test its performance, take
a look at OpenVINO™ samples. All samples supporting the “-d” command-line option
(which stands for “device”) will accept the plugin out-of-the-box. The Benchmark
Application will be a perfect place to start – it presents the optimal performance
of the plugin without the need for additional settings, like the number of requests
or CPU threads. To evaluate the AUTO performance, you can use the following commands:</p>
<p>For unlimited device choice:</p>
<div class="highlight"><div class="highlight"><pre><span></span><span class="n">benchmark_app</span> <span class="err">–</span><span class="n">d</span> <span class="n">AUTO</span> <span class="err">–</span><span class="n">m</span> <span class="o">&lt;</span><span class="n">model</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">i</span> <span class="o">&lt;</span><span class="n">input</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">niter</span> <span class="mi">1000</span></pre></div></div><p>For limited device choice:</p>
<div class="highlight"><div class="highlight"><pre><span></span><span class="n">benchmark_app</span> <span class="err">–</span><span class="n">d</span> <span class="nl">AUTO</span><span class="p">:</span><span class="n">CPU</span><span class="p">,</span><span class="n">GPU</span><span class="p">,</span><span class="n">MYRIAD</span> <span class="err">–</span><span class="n">m</span> <span class="o">&lt;</span><span class="n">model</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">i</span> <span class="o">&lt;</span><span class="n">input</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">niter</span> <span class="mi">1000</span></pre></div></div><p>For more information, refer to the <a class="reference internal" href="../../get-started-guide/samples/cpp-benchmark-tool.html#get-started-samples-cpp-benchmark"><span class="std std-ref">C++</span></a>
or <a class="reference internal" href="../../get-started-guide/samples/python-benchmark-tool.html#get-started-samples-python-benchmark"><span class="std std-ref">Python</span></a> version instructions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default CPU stream is 1 if using “-d AUTO”.</p>
<p>You can use the FP16 IR to work with auto-device.</p>
<p>No demos are yet fully optimized for AUTO, by means of selecting the most suitable device, using the GPU streams/throttling, and so on.</p>
</div>
</section>
<section id="additional-resources">
<h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="automatic-device-selection/debugging-auto-device.html#deploy-infer-debugging-auto-plugin"><span class="std std-ref">Debugging AUTO</span></a></p></li>
<li><p><a class="reference internal" href="multi-device-execution-mode.html#deploy-infer-multi-plugin"><span class="std std-ref">Running on Multiple Devices Simultaneously</span></a></p></li>
<li><p><a class="reference internal" href="../../resources/record-of-openvino-supported-devices.html#resources-supp-devices"><span class="std std-ref">Supported Devices</span></a></p></li>
</ul>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="dynamic-shapes/dynamic-shapes-not-applicable.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="automatic-device-selection/debugging-auto-device.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, Intel®.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>